# ğŸ›’ ShopNow+ : Plateforme E-commerce Big Data

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/)
[![React](https://img.shields.io/badge/React-18.0+-61DAFB.svg)](https://reactjs.org/)
[![Kafka](https://img.shields.io/badge/Apache%20Kafka-3.0+-black.svg)](https://kafka.apache.org/)
[![Spark](https://img.shields.io/badge/Apache%20Spark-3.5+-E25A1C.svg)](https://spark.apache.org/)
[![Hadoop](https://img.shields.io/badge/Hadoop%20HDFS-3.2+-yellow.svg)](https://hadoop.apache.org/)
[![Docker](https://img.shields.io/badge/Docker-Compose-2496ED.svg)](https://www.docker.com/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

**ShopNow+** est une plateforme e-commerce complÃ¨te construite sur une architecture Big Data moderne. Le projet dÃ©montre l'intÃ©gration d'un frontend React, d'un backend Flask, d'Apache Kafka pour la gestion d'Ã©vÃ©nements en temps rÃ©el, de HDFS pour le stockage distribuÃ© et de Spark Streaming pour l'analyse en temps rÃ©el.

---

## ğŸ“‹ Table des matiÃ¨res

- [Architecture](#-architecture)
- [FonctionnalitÃ©s](#-fonctionnalitÃ©s)
- [Technologies](#-technologies)
- [Ã‰quipe](#-Ã©quipe-groupe-6)
- [PrÃ©requis](#-prÃ©requis)
- [Installation](#-installation)
- [Utilisation](#-utilisation)
- [Tests et Validation](#-tests-et-validation)
- [Structure du Projet](#-structure-du-projet)
- [Flux de DonnÃ©es](#-flux-de-donnÃ©es)
- [ArrÃªt](#-arrÃªt)
- [Licence](#-licence)

---

## ğŸ›ï¸ Architecture

L'application est conÃ§ue autour d'une architecture microservices orchestrÃ©e par Docker Compose :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚
â”‚  (Browser)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend   â”‚â”€â”€â”€â”€â”€â–¶â”‚   Backend   â”‚â”€â”€â”€â”€â”€â–¶â”‚    Kafka    â”‚
â”‚   (React)   â”‚      â”‚   (Flask)   â”‚      â”‚   (Broker)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                                  â”‚
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â–¼                     â–¼                     â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   Consumer  â”‚      â”‚    Spark    â”‚      â”‚    HDFS     â”‚
                     â”‚    HDFS     â”‚      â”‚  Streaming  â”‚      â”‚  (Storage)  â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flux de donnÃ©es :

1. **Frontend (React)** : Interface utilisateur oÃ¹ les clients naviguent et passent commandes
2. **Backend (Flask)** : API REST qui gÃ¨re la logique mÃ©tier et produit des Ã©vÃ©nements
3. **Kafka** : Message broker qui transporte les Ã©vÃ©nements de maniÃ¨re asynchrone
4. **Consumer HDFS** : Service Python qui archive tous les Ã©vÃ©nements dans HDFS
5. **Spark Streaming** : Analyse les Ã©vÃ©nements en temps rÃ©el pour gÃ©nÃ©rer des KPIs
6. **HDFS** : Data Lake pour l'archivage long terme des Ã©vÃ©nements

---

## âœ¨ FonctionnalitÃ©s

### ğŸ›ï¸ E-commerce
- Navigation dans un catalogue de produits avec filtres avancÃ©s
- SystÃ¨me de panier dynamique
- Authentification et gestion de profil utilisateur
- Recommandations personnalisÃ©es

### ğŸ“Š Big Data & Analytics
- **5 Ã©vÃ©nements mÃ©tier** capturÃ©s en temps rÃ©el :
  - `produit-consulte` : Consultation d'un produit
  - `article-ajoute` : Ajout au panier
  - `commande-validee` : Validation d'une commande
  - `paiement-accepte` : Paiement acceptÃ©
  - `stock-mis-a-jour` : Mise Ã  jour du stock

- **Analyses temps rÃ©el (Spark Streaming)** :
  - Top 10 des produits les plus consultÃ©s (fenÃªtre glissante 10 min)
  - Chiffre d'affaires en temps rÃ©el (fenÃªtre 5 min)
  - Produits les plus ajoutÃ©s au panier
  - Alertes de rupture de stock (stock < 10)

- **Stockage distribuÃ© (HDFS)** :
  - Archivage automatique de tous les Ã©vÃ©nements
  - Organisation par topic et par date
  - MÃ©canisme de fallback local en cas d'indisponibilitÃ©

---

## ğŸ’» Technologies

| Composant | Technologie | Version |
|-----------|-------------|---------|
| **Frontend** | React, React Router, Axios | 18.x |
| **Backend** | Flask, SQLite | Python 3.9+ |
| **Message Broker** | Apache Kafka, Zookeeper | Wurstmeister |
| **Stockage** | Hadoop HDFS | 3.2.1 |
| **Analyse** | Apache Spark (PySpark) | 3.5.x |
| **Orchestration** | Docker, Docker Compose | - |

---

## ğŸ‘¥ Ã‰quipe (Groupe 6)

- **Gills Daryl KETCHA NZOUNDJI J.** - SpÃ©cialiste Kafka & Backend
- **Narcisse Cabrel TSAFACK FOUEGAP** - SpÃ©cialiste Frontend & Architecture Client
- **FrÃ©dÃ©ric FERNADES DA COSTA** - SpÃ©cialiste Spark & HDFS

---

## ğŸ“¦ PrÃ©requis

Avant de commencer, assurez-vous d'avoir installÃ© :

- **Docker** (version 20.x ou supÃ©rieure)
- **Docker Compose** (version 2.x ou supÃ©rieure)
- **Python 3.9+** (pour le script d'importation du dataset)
- **pip** (gestionnaire de paquets Python)

### VÃ©rification des prÃ©requis

```powershell
# VÃ©rifier Docker
docker --version

# VÃ©rifier Docker Compose
docker-compose --version

# VÃ©rifier Python
python --version
```

---

## ğŸš€ Installation

### Ã‰tape 1 : Cloner le projet

```powershell
git clone https://github.com/JoeDalton318/ShopNow.git
cd ShopNow
```

### Ã‰tape 2 : TÃ©lÃ©charger et importer le dataset

Le projet utilise un dataset de produits. Avant de lancer les conteneurs, vous devez initialiser la base de donnÃ©es.

```powershell
# CrÃ©er un environnement virtuel Python (recommandÃ©)
python -m venv .venv

# Activer l'environnement virtuel
.venv\Scripts\Activate.ps1

# Installer les dÃ©pendances du backend
pip install -r backend/requirements.txt

# Lancer le script d'importation du dataset
cd backend
python import_dataset.py
```

**Ce que fait ce script :**
- TÃ©lÃ©charge le dataset depuis Kaggle (si configurÃ©) ou utilise un dataset local
- CrÃ©e la base de donnÃ©es SQLite `database.db`
- Initialise les tables via `schema.sql`
- InsÃ¨re les produits et catÃ©gories

> **Note :** Si vous n'avez pas configurÃ© Kaggle API, le script utilisera un dataset de dÃ©monstration ou vous demandera de placer manuellement le fichier CSV.

### Ã‰tape 3 : Lancer l'architecture Docker

Retournez Ã  la racine du projet et lancez tous les services :

```powershell
cd ..
docker-compose up -d --build
```

**Temps d'attente :** La premiÃ¨re fois, le tÃ©lÃ©chargement des images et la construction des conteneurs peuvent prendre 5-10 minutes. Attendez environ 30 secondes aprÃ¨s le lancement pour que tous les services soient initialisÃ©s.

### Ã‰tape 4 : VÃ©rifier que tous les conteneurs sont actifs

```powershell
docker ps
```

Vous devriez voir 8 conteneurs en cours d'exÃ©cution :
- `frontend`
- `backend`
- `kafka`
- `zookeeper`
- `kafka-consumer`
- `spark`
- `namenode` (HDFS)
- `datanode` (HDFS)

---

## ğŸŒ Utilisation

Une fois tous les conteneurs dÃ©marrÃ©s, les services suivants sont accessibles :

| Service | URL | Description |
|---------|-----|-------------|
| **Site Web (Frontend)** | [http://localhost:3000](http://localhost:3000) | Interface utilisateur e-commerce |
| **API Backend** | [http://localhost:8000](http://localhost:8000) | API REST (ex: `/produits`) |
| **HDFS NameNode UI** | [http://localhost:9870](http://localhost:9870) | Interface de gestion HDFS |
| **Kafka Broker** | `localhost:9092` | Pour connexion client Kafka |
| **Zookeeper** | `localhost:2181` | Coordination Kafka |

### Naviguer sur le site

1. Ouvrez votre navigateur Ã  [http://localhost:3000](http://localhost:3000)
2. Naviguez dans le catalogue, consultez des produits
3. CrÃ©ez un compte ou connectez-vous
4. Ajoutez des articles au panier
5. Validez une commande

**Chaque action gÃ©nÃ¨re des Ã©vÃ©nements** qui sont capturÃ©s par Kafka, archivÃ©s dans HDFS et analysÃ©s par Spark en temps rÃ©el.

---

## ğŸ§ª Tests et Validation

### 1. VÃ©rifier les analyses Spark en temps rÃ©el

Consultez les logs du conteneur Spark pour voir les tableaux de bord mis Ã  jour :

```powershell
docker logs shopnow_project_20251122-spark-1 --tail 100
```

**RÃ©sultat attendu :** Tableaux affichant les top produits, le CA temps rÃ©el, les alertes stock, etc.

### 2. Inspecter les messages Kafka

Lire les 5 premiers messages du topic `produit-consulte` :

```powershell
docker exec shopnow-kafka kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic produit-consulte --from-beginning --max-messages 5
```

**RÃ©sultat attendu :** Messages JSON avec `id_produit`, `nom_produit`, `timestamp`, `prix`, `user_id`.

### 3. Lister les topics Kafka

```powershell
docker exec shopnow-kafka kafka-topics.sh --list --bootstrap-server localhost:9092
```

**RÃ©sultat attendu :** Liste des 5 topics :
- `article-ajoute`
- `commande-validee`
- `paiement-accepte`
- `produit-consulte`
- `stock-mis-a-jour`

### 4. Explorer les fichiers dans HDFS

Lister l'arborescence complÃ¨te des Ã©vÃ©nements stockÃ©s :

```powershell
docker exec namenode hdfs dfs -ls -R /shopnow/events
```

**RÃ©sultat attendu :** Arborescence organisÃ©e par topic et par date.

### 5. Lire le contenu d'un fichier HDFS

Afficher les 5 premiÃ¨res lignes d'un fichier d'Ã©vÃ©nements :

```powershell
docker exec namenode hdfs dfs -cat /shopnow/events/produit-consulte/2025-12-02.json | Select-Object -First 5
```

**RÃ©sultat attendu :** Lignes JSON (newline-delimited), une par Ã©vÃ©nement.

### 6. VÃ©rifier les logs du consumer HDFS

```powershell
docker logs shopnow_project_20251122-kafka-consumer-1 --tail 20
```

**RÃ©sultat attendu :** Lignes confirmant l'Ã©criture sur HDFS ou le backup local.

### 7. Filtrer les alertes de stock dans Spark

```powershell
docker logs shopnow_project_20251122-spark-1 --tail 50 | Select-String "ALERTE"
```

**RÃ©sultat attendu :** Messages d'alerte si un stock est bas (`nouveau_stock < 10`).

### 8. Tester l'API Backend

```powershell
# Lister tous les produits
Invoke-RestMethod -Uri http://localhost:8000/produits -Method GET

# Obtenir un produit spÃ©cifique
Invoke-RestMethod -Uri http://localhost:8000/produits/1234 -Method GET
```

---

## ğŸ“ Structure du Projet

```
ShopNow/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py                  # Application Flask (API REST)
â”‚   â”œâ”€â”€ kafka_producer.py       # Module d'envoi d'Ã©vÃ©nements Kafka
â”‚   â”œâ”€â”€ import_dataset.py       # Script d'importation du dataset
â”‚   â”œâ”€â”€ schema.sql              # SchÃ©ma de la base de donnÃ©es
â”‚   â”œâ”€â”€ requirements.txt        # DÃ©pendances Python
â”‚   â”œâ”€â”€ Dockerfile              # Image Docker du backend
â”‚   â””â”€â”€ static/images/          # Images des produits
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/         # Composants React rÃ©utilisables
â”‚   â”‚   â”œâ”€â”€ pages/              # Pages de l'application
â”‚   â”‚   â”œâ”€â”€ context/            # Contexts React (Auth, Panier, Theme)
â”‚   â”‚   â”œâ”€â”€ hooks/              # Hooks personnalisÃ©s
â”‚   â”‚   â””â”€â”€ api.js              # Client Axios pour l'API
â”‚   â”œâ”€â”€ public/                 # Fichiers statiques
â”‚   â”œâ”€â”€ package.json            # DÃ©pendances Node.js
â”‚   â””â”€â”€ Dockerfile              # Image Docker du frontend
â”œâ”€â”€ kafka/
â”‚   â”œâ”€â”€ consumer_multi_topics.py # Consumer Python vers HDFS
â”‚   â””â”€â”€ Dockerfile              # Image Docker du consumer
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ spark_streaming_realtime.py # Application Spark Streaming
â”‚   â””â”€â”€ Dockerfile              # Image Docker de Spark
â”œâ”€â”€ docker-compose.yml          # Orchestration de tous les services
â”œâ”€â”€ .gitignore                  # Fichiers ignorÃ©s par Git
â”œâ”€â”€ LICENSE                     # Licence MIT
â””â”€â”€ README.md                   # Ce fichier
```

---

## ğŸ”„ Flux de DonnÃ©es

### Exemple : Consultation d'un produit

1. **Client** : Clique sur un produit dans le catalogue (Frontend React)
2. **Frontend** : Envoie `GET /produits/:id` au Backend
3. **Backend** : 
   - RÃ©cupÃ¨re le produit en base SQLite
   - Enregistre l'historique de consultation
   - Envoie un Ã©vÃ©nement `produit-consulte` Ã  Kafka
4. **Kafka** : Distribue l'Ã©vÃ©nement aux consumers abonnÃ©s
5. **Consumer HDFS** : Archive l'Ã©vÃ©nement dans `/shopnow/events/produit-consulte/{date}.json`
6. **Spark Streaming** : Analyse l'Ã©vÃ©nement et met Ã  jour le "Top 10 produits consultÃ©s"
7. **Frontend** : Affiche les dÃ©tails du produit au client

### Formats de donnÃ©es

- **API REST** : JSON
- **Kafka** : JSON (sÃ©rialisÃ© UTF-8)
- **HDFS** : JSON newline-delimited (1 Ã©vÃ©nement par ligne)
- **Spark** : DataFrames (en mÃ©moire)

---

## ğŸ›‘ ArrÃªt

Pour arrÃªter tous les conteneurs sans supprimer les donnÃ©es :

```powershell
docker-compose stop
```

Pour arrÃªter et supprimer tous les conteneurs, rÃ©seaux et volumes :

```powershell
docker-compose down
```

Pour tout supprimer y compris les volumes (âš ï¸ perte de donnÃ©es HDFS) :

```powershell
docker-compose down -v
```

---

## ğŸ“œ Licence

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

---

## ğŸ™ Remerciements

- Dataset fashion products inspirÃ© de Kaggle
- Architecture basÃ©e sur les bonnes pratiques Big Data

---

## ğŸ“ Contact

Pour toute question ou suggestion, n'hÃ©sitez pas Ã  ouvrir une issue sur GitHub.

**DÃ©veloppÃ© avec â¤ï¸ par le Groupe 6**
